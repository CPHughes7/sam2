{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Train RF-DETR Object Detection on a Custom Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![hf space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SkalskiP/RF-DETR)\n",
        "[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb)\n",
        "[![roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/rf-detr)\n",
        "[![code](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/rf-detr)\n",
        "\n",
        "RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.\n",
        "\n",
        "![rf-detr-coco-rf100-vl-8](https://media.roboflow.com/rfdetr/pareto.png)\n",
        "\n",
        "The RF-DETR family of models stands as the quickest and most precise in object detection across all sizes. RF-DETR has achieved over 60 mAP on the Microsoft COCO benchmark, a leading measure of object detection performance. It also sets new records on RF100-VL, a benchmark that shows how well models adapt to real-world problems beyond standard datasets.\n",
        "\n",
        "The RF-DETR model group includes five sizes: Nano, Small, Medium, Base, and Large. These models offer a range of options for different needs. For example, RF-DETR-Nano is 11 mAP higher than YOLO11-n (on mAP50:95) and runs 0.17 ms faster. Likewise, RF-DETR-Small is 1.8 mAP better than YOLO11-x (the biggest YOLO11 model) and speeds things up by a good 7.77 ms. This wide range of models makes RF-DETR a great choice for many real-world uses, from small devices that need to be super fast to bigger jobs that demand top precision.\n",
        "\n",
        "RF-DETR is small enough to run on edge devices, making it perfect for deployments that need both high accuracy and real-time performance. You can easily get started with any of the RF-DETR models, as they're ready for training in the cloud with Roboflow or through the free RF-DETR Python package."
      ],
      "metadata": {
        "id": "ww8fUIQ8SY2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment setup"
      ],
      "metadata": {
        "id": "qhxdEkemS29m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure API Key\n",
        "\n",
        "To fine-tune RF-DETR, you need to provide your Roboflow API key. Follow these steps:\n",
        "\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy` to copy your private API key.\n",
        "- In Colab, go to the left pane and click on `Secrets` (🔑).\n",
        "    - Store your Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ],
      "metadata": {
        "id": "rT5h00v0TN54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
      ],
      "metadata": {
        "id": "_dGO4a7eTbFX",
        "outputId": "747967d7-c068-4707-af33-abb1ec67310e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret ROBOFLOW_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3843916387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ROBOFLOW_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROBOFLOW_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret ROBOFLOW_API_KEY does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU availability\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "iwzfrPKyTqDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms0ps6ZCT2xs",
        "outputId": "512af8c1-4457-4831-9920-7d797c344882"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 15 17:52:16 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies\n",
        "\n",
        "Installs RF-DETR version 1.2.1 or higher (which includes the new Nano, Small, and Medium checkpoints), along with Supervision for benchmarking and Roboflow for pulling datasets and uploading models to the Roboflow platform."
      ],
      "metadata": {
        "id": "JSbiL1I6T8JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rfdetr==1.2.1 supervision==0.26.1 roboflow"
      ],
      "metadata": {
        "id": "3CbzMY6wITlr",
        "outputId": "3c1b715a-9bb2-4134-da68-6b41cf520b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/266.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.8/372.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download example data\n",
        "\n",
        "Downloads example images for testing. You can use these or replace them with your own images."
      ],
      "metadata": {
        "id": "hzulEARVVZxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg"
      ],
      "metadata": {
        "id": "8DCCwexcU6gO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Pre-trained COCO Model\n",
        "\n",
        "Runs inference on an example image using a pretrained RF-DETR Medium model (trained on COCO). To use a different model size, simply replace `RFDETRMedium` with `RFDETRNano`, `RFDETRSmall`, `RFDETRBase` or `RFDETRLarge` as needed."
      ],
      "metadata": {
        "id": "wGfBHjJwWNEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from rfdetr import RFDETRMedium\n",
        "from rfdetr.util.coco_classes import COCO_CLASSES\n",
        "\n",
        "image = Image.open(\"dog-2.jpeg\")\n",
        "\n",
        "model = RFDETRMedium(resolution=640)\n",
        "model.optimize_for_inference()\n",
        "\n",
        "detections = model.predict(image, threshold=0.5)\n",
        "\n",
        "color = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n",
        "    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
        "thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
        "\n",
        "bbox_annotator = sv.BoxAnnotator(color=color, thickness=thickness)\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=color,\n",
        "    text_color=sv.Color.BLACK,\n",
        "    text_scale=text_scale,\n",
        "    smart_position=True\n",
        ")\n",
        "\n",
        "labels = [\n",
        "    f\"{COCO_CLASSES[class_id]} {confidence:.2f}\"\n",
        "    for class_id, confidence\n",
        "    in zip(detections.class_id, detections.confidence)\n",
        "]\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = bbox_annotator.annotate(annotated_image, detections)\n",
        "annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n",
        "annotated_image.thumbnail((800, 800))\n",
        "annotated_image"
      ],
      "metadata": {
        "id": "PtOufRspVekp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset from Roboflow Universe\n",
        "\n",
        "RF-DETR expects the dataset to be in COCO format. Divide your dataset into three subdirectories: `train`, `valid`, and `test`. Each subdirectory should contain its own `_annotations.coco.json` file that holds the annotations for that particular split, along with the corresponding image files. Below is an example of the directory structure:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── train/\n",
        "│   ├── _annotations.coco.json\n",
        "│   ├── image1.jpg\n",
        "│   ├── image2.jpg\n",
        "│   └── ... (other image files)\n",
        "├── valid/\n",
        "│   ├── _annotations.coco.json\n",
        "│   ├── image1.jpg\n",
        "│   ├── image2.jpg\n",
        "│   └── ... (other image files)\n",
        "└── test/\n",
        "    ├── _annotations.coco.json\n",
        "    ├── image1.jpg\n",
        "    ├── image2.jpg\n",
        "    └── ... (other image files)\n",
        "```\n",
        "\n",
        "[Roboflow](https://roboflow.com/annotate) allows you to create object detection datasets from scratch or convert existing datasets from formats like YOLO, and then export them in COCO JSON format for training. You can also explore [Roboflow Universe](https://universe.roboflow.com/) to find pre-labeled datasets for a range of use cases."
      ],
      "metadata": {
        "id": "MK6zkzY1lPtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import download_dataset\n",
        "\n",
        "dataset = download_dataset(\"https://universe.roboflow.com/roboflow-jvuqo/basketball-player-detection-2/13\", \"coco\")"
      ],
      "metadata": {
        "id": "hQkMUyB0lROT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train RF-DETR on custom dataset\n",
        "\n",
        "### Choose the right `batch_size`\n",
        "\n",
        "Different GPUs have different amounts of VRAM (video memory), which limits how much data they can handle at once during training. To make training work well on any machine, you can adjust two settings: `batch_size` and `grad_accum_steps`. These control how many samples are processed at a time. The key is to keep their product equal to 16 — that’s our recommended total batch size. For example, on powerful GPUs like the A100, set `batch_size=16` and `grad_accum_steps=1`. On smaller GPUs like the T4, use `batch_size=4` and `grad_accum_steps=4`. We use a method called gradient accumulation, which lets the model simulate training with a larger batch size by gradually collecting updates before adjusting the weights."
      ],
      "metadata": {
        "id": "vmT8f_bAq3zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rfdetr import RFDETRMedium\n",
        "\n",
        "model = RFDETRMedium()\n",
        "\n",
        "model.train(dataset_dir=dataset.location, epochs=10, batch_size=8, grad_accum_steps=2)"
      ],
      "metadata": {
        "id": "1UvmuIammK9s",
        "outputId": "a817c475-9c9a-4feb-991a-1f18fff5deeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rf-detr-medium.pth: 100%|██████████| 386M/386M [00:09<00:00, 40.8MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n",
            "Using patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n",
            "Loading pretrain weights\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3259934861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFDETRMedium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "Image.open(\"/content/output/metrics_plot.png\")"
      ],
      "metadata": {
        "id": "65A4MXYtB94O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content/output"
      ],
      "metadata": {
        "id": "mYLZ3btmqygA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Fine-tuned RF-DETR Model"
      ],
      "metadata": {
        "id": "X_9c113E39QP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before benchmarking the model, we need to load the best saved checkpoint. To ensure it fits on the GPU, we first need to free up GPU memory. This involves deleting any remaining references to previously used objects, triggering Python’s garbage collector, and clearing the CUDA memory cache."
      ],
      "metadata": {
        "id": "jRE_csKqxC9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import weakref\n",
        "\n",
        "def cleanup_gpu_memory(obj=None, verbose: bool = False):\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        if verbose:\n",
        "            print(\"[INFO] CUDA is not available. No GPU cleanup needed.\")\n",
        "        return\n",
        "\n",
        "    def get_memory_stats():\n",
        "        allocated = torch.cuda.memory_allocated()\n",
        "        reserved = torch.cuda.memory_reserved()\n",
        "        return allocated, reserved\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    if verbose:\n",
        "        alloc, reserv = get_memory_stats()\n",
        "        print(f\"[Before] Allocated: {alloc / 1024**2:.2f} MB | Reserved: {reserv / 1024**2:.2f} MB\")\n",
        "\n",
        "    # Ensure we drop all strong references\n",
        "    if obj is not None:\n",
        "        ref = weakref.ref(obj)\n",
        "        del obj\n",
        "        if ref() is not None and verbose:\n",
        "            print(\"[WARNING] Object not fully garbage collected yet.\")\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    if verbose:\n",
        "        alloc, reserv = get_memory_stats()\n",
        "        print(f\"[After]  Allocated: {alloc / 1024**2:.2f} MB | Reserved: {reserv / 1024**2:.2f} MB\")"
      ],
      "metadata": {
        "id": "0TBa42oujIfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanup_gpu_memory(model, verbose=True)"
      ],
      "metadata": {
        "id": "VSei0WZ8qt_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the best-performing model from the `checkpoint_best_total.pth` file using the `RFDETRMedium` class. This checkpoint contains the trained weights from our most successful training run. After loading, we call `optimize_for_inference()`, which prepares the model for efficient inference."
      ],
      "metadata": {
        "id": "B_dvSiE8xUqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RFDETRMedium(pretrain_weights=\"/content/output/checkpoint_best_total.pth\")\n",
        "model.optimize_for_inference()"
      ],
      "metadata": {
        "id": "5xJddevbqxrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "\n",
        "ds = sv.DetectionDataset.from_coco(\n",
        "    images_directory_path=f\"{dataset.location}/test\",\n",
        "    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n",
        ")"
      ],
      "metadata": {
        "id": "xm-lmRWLswO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "from tqdm import tqdm\n",
        "from supervision.metrics import MeanAveragePrecision\n",
        "\n",
        "targets = []\n",
        "predictions = []\n",
        "\n",
        "for path, image, annotations in tqdm(ds):\n",
        "    image = Image.open(path)\n",
        "    detections = model.predict(image, threshold=0)\n",
        "\n",
        "    targets.append(annotations)\n",
        "    predictions.append(detections)"
      ],
      "metadata": {
        "id": "szxs3PZsBVxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_metric = MeanAveragePrecision()\n",
        "map_result = map_metric.update(predictions, targets).compute()\n",
        "print(map_result)"
      ],
      "metadata": {
        "id": "fxqvXOQcsRF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference with Fine-tuned RF-DETR Model"
      ],
      "metadata": {
        "id": "xT1tPwZS_-6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rfdetr import RFDETRBase\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "\n",
        "path, image, annotations = ds[0]\n",
        "image = Image.open(path)\n",
        "\n",
        "detections = model.predict(image, threshold=0.5)\n",
        "\n",
        "text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
        "thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
        "color = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
        "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "\n",
        "bbox_annotator = sv.BoxAnnotator(color=color,thickness=thickness)\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=color,\n",
        "    text_color=sv.Color.BLACK,\n",
        "    text_scale=text_scale)\n",
        "\n",
        "annotations_labels = [\n",
        "    f\"{ds.classes[class_id]}\"\n",
        "    for class_id\n",
        "    in annotations.class_id\n",
        "]\n",
        "\n",
        "detections_labels = [\n",
        "    f\"{ds.classes[class_id]} {confidence:.2f}\"\n",
        "    for class_id, confidence\n",
        "    in zip(detections.class_id, detections.confidence)\n",
        "]\n",
        "\n",
        "annotation_image = image.copy()\n",
        "annotation_image = bbox_annotator.annotate(annotation_image, annotations)\n",
        "annotation_image = label_annotator.annotate(annotation_image, annotations, annotations_labels)\n",
        "\n",
        "detections_image = image.copy()\n",
        "detections_image = bbox_annotator.annotate(detections_image, detections)\n",
        "detections_image = label_annotator.annotate(detections_image, detections, detections_labels)\n",
        "\n",
        "sv.plot_images_grid(images=[annotation_image, detections_image], grid_size=(1, 2), titles=[\"Annotation\", \"Detection\"])"
      ],
      "metadata": {
        "id": "msor_5HgAkm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q inference-gpu==0.51.7"
      ],
      "metadata": {
        "id": "6GTpfSq3rm5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inference import get_model\n",
        "\n",
        "MODEL_ID = \"basketball-player-detection-2/13\"\n",
        "model_rf = get_model(model_id=MODEL_ID, api_key = userdata.get(\"ROBOFLOW_API_KEY\"))"
      ],
      "metadata": {
        "id": "A_g82Ysur2wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "from PIL import Image\n",
        "\n",
        "path, image, annotations = ds[0]\n",
        "image = Image.open(path)\n",
        "\n",
        "result = model_rf.infer(image, confidence=0.3)[0]\n",
        "detections = sv.Detections.from_inference(result)\n",
        "\n",
        "text_scale = sv.calculate_optimal_text_scale(resolution_wh=image.size)\n",
        "thickness = sv.calculate_optimal_line_thickness(resolution_wh=image.size)\n",
        "color = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
        "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "\n",
        "bbox_annotator = sv.BoxAnnotator(color=color,thickness=thickness)\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    color=color,\n",
        "    text_color=sv.Color.BLACK,\n",
        "    text_scale=text_scale)\n",
        "\n",
        "annotations_labels = [\n",
        "    f\"{ds.classes[class_id]}\"\n",
        "    for class_id\n",
        "    in annotations.class_id\n",
        "]\n",
        "\n",
        "annotation_image = image.copy()\n",
        "annotation_image = bbox_annotator.annotate(annotation_image, annotations)\n",
        "annotation_image = label_annotator.annotate(annotation_image, annotations, annotations_labels)\n",
        "\n",
        "detections_image = image.copy()\n",
        "detections_image = bbox_annotator.annotate(detections_image, detections)\n",
        "detections_image = label_annotator.annotate(detections_image, detections)\n",
        "\n",
        "sv.plot_images_grid(images=[annotation_image, detections_image], grid_size=(1, 2), titles=[\"Annotation\", \"Detection\"])"
      ],
      "metadata": {
        "id": "vRei_qUisZgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "  <p>\n",
        "    Looking for more tutorials or have questions?\n",
        "    Check out our <a href=\"https://github.com/roboflow/notebooks\">GitHub repo</a> for more notebooks,\n",
        "    or visit our <a href=\"https://discord.gg/GbfgXGJ8Bk\">discord</a>.\n",
        "  </p>\n",
        "  \n",
        "  <p>\n",
        "    <strong>If you found this helpful, please consider giving us a ⭐\n",
        "    <a href=\"https://github.com/roboflow/notebooks\">on GitHub</a>!</strong>\n",
        "  </p>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "1P1dYTMTYGT3"
      }
    }
  ]
}